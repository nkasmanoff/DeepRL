{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor critic methods are a way of reducing the amount of variance from a policy gradient. The idea is that by using two networks, or at least two different function approximators, one can learn two more easily digestible curriculums which can help an agent successfully master whatever environment it is currently in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a discrete actor critic now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate actor critic algorithms mathematically, recall the policy gradient is  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_{i,t} | s_{i,t}) G_{i,t}$$\n",
    "\n",
    "Where $G_{i,t}$ represents some advantage/reward-to-go this policy takes has compared to the other possible actions at that state. In the regular old policy-gradient, $G$ is simply the total remaining rewards to go, implying that this action could either be really good or really bad depending on the value of $G$. \n",
    "\n",
    "Unfortunately, there are many caveats associated with $G$ simply being the total reward to go. For one, this value is  insensitive to a relative reward difference, meaning that if you were to simply add $40$ to all possible rewards the agent receives, you could end up with a very different policy! Good policies should not be sensitive to scalar shifts. Clearly, we have some work to do... \n",
    "\n",
    "There are a bunch of ways to fix $G_{i,t}$ for variance reduction, where one of the most popular (and already implemented) was a baseline which corresponded to the mean reward. Additionally, we could see how something like an expected reward to go, would be a better estimate and variance reduction technique. We'll see that shortly.\n",
    "\n",
    "With a baseline, the policy gradient is \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_{i,t} | s_{i,t})( G_{i,t} - b(s_{i,t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is no reason for this baseline to be a constant number! In fact, it would be a great idea if this baseline could also be a function of the state observation $s_t$. At the same time, this value is starting to look a lot like the reinforcement learning concepts we considered at the beginning of this course, mainly, the $Q$ function, the $V$ function, and the $A$ function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $Q$ value is the expected reward to go, given a state and action. \n",
    "\n",
    "$Q(s_t,a_t) = E[G_t | (s_t, a_t)]$\n",
    "\n",
    "The $V$ value is the average $Q$ values in a given state, over possible actions. \n",
    "\n",
    "$V(s_t) = \\sum_a Q(s,a)$\n",
    "\n",
    "and $A$ represents the advantage a particular $Q$ value has over the average rewards to go in a state. It is simply \n",
    "\n",
    "$A = Q - V$\n",
    "\n",
    "\n",
    "Any three of these functions, (Q,V,A) could be represented as some combination of the other one, and thus could be used as the approximator we plug into the AC algorithm.\n",
    "\n",
    "In any case we can leverage Bellman optimality conditions, and use that as a method to fit this critic network. \n",
    "\n",
    "The code for this is below. More specifically, how to make an Actor-Critic Algorithm that trains an actor-critic agent in a continuous action space.\n",
    "\n",
    "Spoiler alert: This algorithm won't work! But that just gives more motivation to making my own implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym \n",
    "#from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two networks, with two hidden layers. \n",
    "\n",
    "#we'll have an actor network, and a critic network.\n",
    "\n",
    "class GenericNetwork(nn.Module):\n",
    "    # To use for both networks \n",
    "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,\n",
    "                n_actions):\n",
    "        super(GenericNetwork,self).__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.fc2_dims,self.n_actions)\n",
    "        \n",
    "        #basis network for both A and C, # of output will change!\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "        self.to(self.device)\n",
    "    #now the forward pass!\n",
    "    \n",
    "    def forward(self,state): \n",
    "        #state and obs are interchangeable, not a POMDP setting...\n",
    "        state = torch.Tensor(state).to(self.device)        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #no activation, handled later \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,alpha,beta,input_dims,gamma=0.99, n_actions = 4, \n",
    "                layer1_size=64,layer2_size = 64):\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = None # log probability of selecting an action \n",
    "        self.actor = GenericNetwork(alpha,input_dims,layer1_size,layer2_size,n_actions = n_actions)\n",
    "        self.critic = GenericNetwork(beta,input_dims,layer1_size,layer2_size,n_actions = 1) \n",
    "          \n",
    "    def choose_action(self,observation):\n",
    "        probabilities = F.softmax(self.actor.forward(observation))\n",
    "        # now we need to force these values to make sense, i.e make std real and non-zero. \n",
    "        action_probs = torch.distributions.Categorical(probabilities) # probability distribuition dictated by policy network\n",
    "        #draw sample from it \n",
    "        action = action_probs.sample()\n",
    "        self.log_probs = action_probs.log_prob(action).to(self.actor.device)\n",
    "\n",
    "        return action.item() #not a tensor!\n",
    "    \n",
    "    def learn(self,state,reward,new_state,done):\n",
    "        #temporal difference style, using state at current and next state.\n",
    "        #PG doesn't use that, Monte-carlo style, which accumulates rewards over the episode. \n",
    "        #This is just a state by state \n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        \n",
    "        critic_value_ = self.critic.forward(new_state) #Q value of next state for TD critic loss\n",
    "        critic_value = self.critic.forward(state) #Q val at current state\n",
    "        \n",
    "        reward = torch.Tensor([reward]).to(self.actor.device)\n",
    "        \n",
    "        delta = reward + self.gamma* critic_value_*(1-int(done))  - critic_value#loss!\n",
    "            #y_i - Vphi i , and this is the same as advantage\n",
    "        \n",
    "        actor_loss = -self.log_probs * delta\n",
    "        \n",
    "        critic_loss = delta **2  #minimize \n",
    "        \n",
    "        #now sum the two, and backpropagate\n",
    "        \n",
    "        (actor_loss + critic_loss).backward() #can't do two backward passes at once, but these losses\n",
    "        #are inpdt so it will update separately :) \n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0 score  -155.294926162938\n",
      "Episode  1 score  -463.31696872876677\n",
      "Episode  2 score  -269.9806020397442\n",
      "Episode  3 score  -222.1380770683628\n",
      "Episode  4 score  -77.2427193647535\n",
      "Episode  5 score  -179.60248956841488\n",
      "Episode  6 score  -84.01171140087416\n",
      "Episode  7 score  -159.23978710373774\n",
      "Episode  8 score  -343.6829701200198\n",
      "Episode  9 score  -128.76537545076116\n",
      "Episode  10 score  -182.73255286395954\n",
      "Episode  11 score  -178.9495756504177\n",
      "Episode  12 score  -215.93856563599587\n",
      "Episode  13 score  -476.20631889559934\n",
      "Episode  14 score  -304.41225215250466\n",
      "Episode  15 score  -140.03210257995278\n",
      "Episode  16 score  -156.1264413355085\n",
      "Episode  17 score  -284.13490710097705\n",
      "Episode  18 score  -155.19633604806018\n",
      "Episode  19 score  -171.55124486193438\n",
      "Episode  20 score  -431.99736492483146\n",
      "Episode  21 score  -156.87072994400538\n",
      "Episode  22 score  -291.8766567558681\n",
      "Episode  23 score  -67.99308238720542\n",
      "Episode  24 score  -164.81553046182154\n",
      "Episode  25 score  -138.89031963821634\n",
      "Episode  26 score  -184.5180389324986\n",
      "Episode  27 score  -149.3231138899443\n",
      "Episode  28 score  -209.11628823979896\n",
      "Episode  29 score  -398.7530577711071\n",
      "Episode  30 score  -544.4479825491012\n",
      "Episode  31 score  -348.49281129578196\n",
      "Episode  32 score  -28.965771580702025\n",
      "Episode  33 score  -358.7124654938364\n",
      "Episode  34 score  -119.52844419889485\n",
      "Episode  35 score  -280.52039236637444\n",
      "Episode  36 score  -443.7455294764471\n",
      "Episode  37 score  -307.89535221591336\n",
      "Episode  38 score  -285.1974366294027\n",
      "Episode  39 score  -250.42172475893878\n",
      "Episode  40 score  -387.10329007792365\n",
      "Episode  41 score  -181.1863493552503\n",
      "Episode  42 score  -56.87696508887481\n",
      "Episode  43 score  -258.1605604541636\n",
      "Episode  44 score  -30.670588592354264\n",
      "Episode  45 score  -303.19292932415226\n",
      "Episode  46 score  -142.0801776815705\n",
      "Episode  47 score  -174.29049725708728\n",
      "Episode  48 score  -426.1124168505928\n",
      "Episode  49 score  -301.90400944374755\n",
      "Episode  50 score  -260.423472147008\n",
      "Episode  51 score  -584.813779404858\n",
      "Episode  52 score  -335.5830409728506\n",
      "Episode  53 score  -90.5136383041639\n",
      "Episode  54 score  -100.56584581201342\n",
      "Episode  55 score  -77.35984440092655\n",
      "Episode  56 score  -350.69908722760965\n",
      "Episode  57 score  -271.6259680536084\n",
      "Episode  58 score  -350.0211035237442\n",
      "Episode  59 score  -122.83892244892198\n",
      "Episode  60 score  -132.35758710045653\n",
      "Episode  61 score  -290.97924562999515\n",
      "Episode  62 score  -259.71540435732084\n",
      "Episode  63 score  -165.47851765633766\n",
      "Episode  64 score  -316.6492545420902\n",
      "Episode  65 score  -244.8577479578115\n",
      "Episode  66 score  -79.47775566437356\n",
      "Episode  67 score  -444.76460271029396\n",
      "Episode  68 score  -414.05225695327226\n",
      "Episode  69 score  -261.4119402754685\n",
      "Episode  70 score  -425.6976021885067\n",
      "Episode  71 score  -270.6259522990355\n",
      "Episode  72 score  -365.55300879750916\n",
      "Episode  73 score  -163.05950130920598\n",
      "Episode  74 score  -199.52137209189732\n",
      "Episode  75 score  -142.7935067084602\n",
      "Episode  76 score  -132.76565663007898\n",
      "Episode  77 score  -201.0012807012717\n",
      "Episode  78 score  -347.299688711724\n",
      "Episode  79 score  -268.36881480635907\n",
      "Episode  80 score  -212.21953083559202\n",
      "Episode  81 score  -459.31344191258626\n",
      "Episode  82 score  -567.9066235789983\n",
      "Episode  83 score  -187.58707239836136\n",
      "Episode  84 score  -94.20623408929971\n",
      "Episode  85 score  -424.25473945304054\n",
      "Episode  86 score  -234.4375212743684\n",
      "Episode  87 score  -473.18877476017065\n",
      "Episode  88 score  -391.08556652086486\n",
      "Episode  89 score  -410.9806065885555\n",
      "Episode  90 score  -219.80798424030223\n",
      "Episode  91 score  -383.74692847453633\n",
      "Episode  92 score  -201.54264828099113\n",
      "Episode  93 score  -315.5909970269791\n",
      "Episode  94 score  -247.6856295351233\n",
      "Episode  95 score  -109.07151783733637\n",
      "Episode  96 score  -227.68861139661038\n",
      "Episode  97 score  -118.45578833499647\n",
      "Episode  98 score  -281.5127574764982\n",
      "Episode  99 score  -481.6775167089885\n"
     ]
    }
   ],
   "source": [
    "# now the main\n",
    "\n",
    "agent = Agent(alpha = 0.00005, beta = .00001, input_dims = [8], gamma=0.99,\n",
    "             layer1_size = 256, layer2_size=256)\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "score_history = []\n",
    "num_episodes = 2000\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    #now the game!\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        env.render()\n",
    "        observation_,reward, done, _ = env.step(action)\n",
    "        agent.learn(observation,reward,observation_, done)\n",
    "        \n",
    "        observation = observation_        \n",
    "        score += reward\n",
    "    score_history.append(score)\n",
    "    \n",
    "    print(\"Episode \", i, 'score ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works, I should feel good. I very quickly was able to adjust an architecture on the fly in order to switch from a continuous to discrete action space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why doesn't this win? It only ever learns to minimize the negative reward. \n",
    "\n",
    "\n",
    "It learns quite hilariously, that if it stays at the bottom, the negative reward is minimized!\n",
    "\n",
    "This is just one example of why AC and RL in general is a shaky tool!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
