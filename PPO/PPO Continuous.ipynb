{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again all credit to https://github.com/nikhilbarhate99/PPO-PyTorch\n",
    "\n",
    "Before I copy and paste in that code for the continuous PPO however, I want to try and guess what I have to do. \n",
    "\n",
    "In the act action in the actor critic class, it should be a normal distribution, not a continuous. Additionally, the output of the actor should be a mean and standard deviation, so two values? \n",
    "\n",
    "No. It's as many values as are in the distribution, so a multivariate gaussian corresponding to 2x the number of actions.\n",
    "\n",
    "One of these actions is the mean, and can be any value. The standard deviation has to be greater than 0, so whatever the network outputs, take the exponential of it.\n",
    "\n",
    "\n",
    "It appears there are some differences between my guess and the output, mainly that the std is a pre-defined value, and that the mean is bounded between -1 and 1. Let's see how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    For the old policy, collect a tuple of (states,actions,logprobs,rewards,done)\n",
    "    which gives information on how this old policy performs, and can be immediately compared to the candidate one. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # action mean range -1 to 1\n",
    "        self.actor =  nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, action_dim),\n",
    "                nn.Tanh() \n",
    "                )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1)\n",
    "                )\n",
    "        self.action_var = torch.full((action_dim,), action_std*action_std).to(device) #actor takes actions according to a multi-variate gaussian. Network finds the means, \n",
    "        #standard deviations are given like this. \n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        action_mean = self.actor(state)\n",
    "        cov_mat = torch.diag(self.action_var).to(device)\n",
    "        \n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample() #sample actions from multivariate. \n",
    "        action_logprob = dist.log_prob(action) #convert to log probability scores of these actions (log pi) vector\n",
    "        \n",
    "        #save these things to the memory. This is for all the OLD actions and logprobs, and are immedaitely compared to how the candidate policy performs on the \n",
    "        # memory saved state\n",
    "        memory.states.append(state) \n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "        \n",
    "        return action.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):   \n",
    "        #Compare the new policies predictions\n",
    "        # Inputs a state and action, outputsa log probability and entropy. \n",
    "        action_mean = self.actor(state)\n",
    "        \n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        \n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        # Hyper parameters\n",
    "        \n",
    "        \n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state, memory):\n",
    "        # select an acion from the old policy + memory . \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert lists to tensors\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # The new state values and log probabilities under the candidate policy change.  :\n",
    "            # The old actions are also inputted because that provides the dist entropy bonus aka encourage exploration. \n",
    "            \n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()    # advantage value \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0.0003 (0.9, 0.999)\n",
      "Episode 20 \t Avg length: 211 \t Avg reward: -102\n",
      "Episode 40 \t Avg length: 582 \t Avg reward: -90\n",
      "Episode 60 \t Avg length: 728 \t Avg reward: -83\n",
      "Episode 80 \t Avg length: 933 \t Avg reward: -73\n",
      "Episode 100 \t Avg length: 1215 \t Avg reward: -64\n",
      "Episode 120 \t Avg length: 1146 \t Avg reward: -68\n",
      "Episode 140 \t Avg length: 1427 \t Avg reward: -53\n",
      "Episode 160 \t Avg length: 1428 \t Avg reward: -50\n",
      "Episode 180 \t Avg length: 1283 \t Avg reward: -52\n",
      "Episode 200 \t Avg length: 1292 \t Avg reward: -47\n",
      "Episode 220 \t Avg length: 1427 \t Avg reward: -26\n",
      "Episode 240 \t Avg length: 1186 \t Avg reward: -28\n",
      "Episode 260 \t Avg length: 1061 \t Avg reward: -44\n",
      "Episode 280 \t Avg length: 1014 \t Avg reward: -41\n",
      "Episode 300 \t Avg length: 1114 \t Avg reward: -12\n",
      "Episode 320 \t Avg length: 1168 \t Avg reward: 7\n",
      "Episode 340 \t Avg length: 1178 \t Avg reward: 6\n",
      "Episode 360 \t Avg length: 1072 \t Avg reward: 18\n",
      "Episode 380 \t Avg length: 1095 \t Avg reward: 34\n",
      "Episode 400 \t Avg length: 1199 \t Avg reward: 53\n",
      "Episode 420 \t Avg length: 1414 \t Avg reward: 101\n",
      "Episode 440 \t Avg length: 1361 \t Avg reward: 96\n",
      "Episode 460 \t Avg length: 1386 \t Avg reward: 104\n",
      "Episode 480 \t Avg length: 985 \t Avg reward: 40\n",
      "Episode 500 \t Avg length: 1418 \t Avg reward: 120\n",
      "Episode 520 \t Avg length: 1335 \t Avg reward: 114\n",
      "Episode 540 \t Avg length: 1077 \t Avg reward: 56\n",
      "Episode 560 \t Avg length: 1177 \t Avg reward: 83\n",
      "Episode 580 \t Avg length: 1234 \t Avg reward: 100\n",
      "Episode 600 \t Avg length: 1097 \t Avg reward: 78\n",
      "Episode 620 \t Avg length: 1099 \t Avg reward: 82\n",
      "Episode 640 \t Avg length: 1284 \t Avg reward: 104\n",
      "Episode 660 \t Avg length: 1261 \t Avg reward: 110\n",
      "Episode 680 \t Avg length: 1159 \t Avg reward: 97\n",
      "Episode 700 \t Avg length: 1386 \t Avg reward: 134\n",
      "Episode 720 \t Avg length: 1312 \t Avg reward: 118\n",
      "Episode 740 \t Avg length: 1394 \t Avg reward: 138\n",
      "Episode 760 \t Avg length: 1318 \t Avg reward: 120\n",
      "Episode 780 \t Avg length: 1448 \t Avg reward: 157\n",
      "Episode 800 \t Avg length: 1293 \t Avg reward: 125\n",
      "Episode 820 \t Avg length: 1137 \t Avg reward: 93\n",
      "Episode 840 \t Avg length: 1256 \t Avg reward: 104\n",
      "Episode 860 \t Avg length: 1213 \t Avg reward: 100\n",
      "Episode 880 \t Avg length: 1270 \t Avg reward: 116\n",
      "Episode 900 \t Avg length: 1030 \t Avg reward: 72\n",
      "Episode 920 \t Avg length: 1294 \t Avg reward: 129\n",
      "Episode 940 \t Avg length: 1315 \t Avg reward: 134\n",
      "Episode 960 \t Avg length: 1159 \t Avg reward: 84\n",
      "Episode 980 \t Avg length: 1379 \t Avg reward: 147\n",
      "Episode 1000 \t Avg length: 937 \t Avg reward: 52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b6f5ac4a1d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Running policy_old:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# choose an action according to the old policy, save this action, log prob, etc. to memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-36450d624fde>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, memory)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# select an acion from the old policy + memory .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a415b18b0bad>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, memory)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sample actions from multivariate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0maction_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#convert to log probability scores of these actions (log pi) vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#save these things to the memory. This is for all the OLD actions and logprobs, and are immedaitely compared to how the candidate policy performs on the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mhalf_log_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhalf_log_det\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_batch_mahalanobis\u001b[0;34m(bL, bx)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mflat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_L\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = c x b x n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mflat_x_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = b x n x c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mM_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangular_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_x_swap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = b x c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_swap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = c x b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "############## Hyperparameters ##############\n",
    "env_name = \"BipedalWalker-v2\"\n",
    "#env_name = \"LunarLanderContinuous-v2\"\n",
    "render = False\n",
    "solved_reward = 300         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 10000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "update_timestep = 4000      # update policy every n timesteps\n",
    "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0003                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "\n",
    "# creating environment\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)     # choose an action according to the old policy, save this action, log prob, etc. to memory. \n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_solved_{}.pth'.format(env_name))\n",
    "        break\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_{}.pth'.format(env_name))\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = int(avg_length/log_interval)\n",
    "        running_reward = int((running_reward/log_interval))\n",
    "\n",
    "        print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3643321 , -0.21307534, -0.20152923,  0.0363022 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of the test loop for once a model is already trained and ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 1\tReward: 194\n",
      "Episode: 2\tReward: 177\n",
      "Episode: 3\tReward: 174\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def test():\n",
    "    ############## Hyperparameters ##############\n",
    "    env_name = \"BipedalWalker-v2\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    n_episodes = 3          # num of episodes to run\n",
    "    max_timesteps = 1500    # max timesteps in one episode\n",
    "    render = True           # render the environment\n",
    "    save_gif = False        # png images are saved in gif folder\n",
    "    \n",
    "    # filename and directory to load model from\n",
    "    filename = \"PPO_continuous_\" +env_name+ \".pth\"\n",
    "    directory = \"\"\n",
    "\n",
    "    action_std = 0.5        # constant std for action distribution (Multivariate Normal)\n",
    "    K_epochs = 80           # update policy for K epochs\n",
    "    eps_clip = 0.2          # clip parameter for PPO\n",
    "    gamma = 0.99            # discount factor\n",
    "    \n",
    "    lr = 0.0003             # parameters for Adam optimizer\n",
    "    betas = (0.9, 0.999)\n",
    "    #############################################\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    #load in best model \n",
    "    ppo.policy_old.load_state_dict(torch.load(directory+filename))\n",
    "    \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            action = ppo.select_action(state, memory)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if save_gif:\n",
    "                 img = env.render(mode = 'rgb_array')\n",
    "                 img = Image.fromarray(img)\n",
    "                 img.save('./gif/{}.jpg'.format(t))  \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "        ep_reward = 0\n",
    "        env.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
