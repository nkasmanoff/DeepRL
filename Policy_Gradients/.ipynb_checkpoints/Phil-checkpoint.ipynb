{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically a copy-paste of https://www.youtube.com/watch?v=GOBvUA9lK1Q&t. But I'm obviously not perfect, so I may get a little nit-picky if there are things I want to add/delete. I'll try my best to document all I can. \n",
    "\n",
    "\n",
    "To start, we're using the Lunar Lander Open AI Gym Environment https://gym.openai.com/envs/LunarLander-v2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym \n",
    "#from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
    "        super(PolicyNetwork,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.lr = lr\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims,self.n_actions)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,observation):\n",
    "        state = torch.Tensor(observation).to(self.device)\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #no activation, handled later\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "#Now the agent class. We express things in terms of classes and objects, a fundamental building block of \n",
    "# OOP and comp sci. \n",
    "\n",
    "class Agent(object):\n",
    "    #contains policy network and more!\n",
    "    def __init__(self, lr, input_dims, gamma = 0.99, n_actions = 4, l1_size = 256, l2_size = 256):\n",
    "    \n",
    "        self.gamma = gamma\n",
    "        self.reward_memory = [] #way of keeping track of rewards \n",
    "        self.action_memory = [] # and actions the agent took\n",
    "        \n",
    "        self.policy = PolicyNetwork(lr, input_dims, l1_size, l2_size, n_actions) #probability distribution used by the agent to select actions, given an observation/state. \n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        probabilities = F.softmax(self.policy.forward(observation))\n",
    "        \n",
    "        #now calculate an actual distribution from this. \n",
    "        \n",
    "        action_probs = torch.distributions.Categorical(probabilities) # probability distribuition dictated by policy network\n",
    "        \n",
    "        # now we pick an action using sample method.\n",
    "        \n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        #keep track of action log probability... log pi!  You know this!\n",
    "        \n",
    "        log_probs = action_probs.log_prob(action) #clearly this is a Pytorch specific method of how to select. \n",
    "        #looks like it takes distribution, and calculates log probability of that specific action. cool. \n",
    "        \n",
    "        \n",
    "        #now save. \n",
    "        \n",
    "        self.action_memory.append(log_probs)\n",
    "        \n",
    "        #return an action\n",
    "        \n",
    "        return action.item() #this item is an integer, now able to get fed into gym env. \n",
    "    \n",
    "    def store_rewards(self,reward):\n",
    "        self.reward_memory.append(reward) #why not just do this in main? idk, it's a little more convenient. \n",
    "        \n",
    "        \n",
    "    def learn(self):\n",
    "        #heart of the problem. \n",
    "        self.policy.optimizer.zero_grad()\n",
    "        \n",
    "        #It learns at the end of the episode, which is not a good thing! Would want an average of future returns, not per ep. \n",
    "        G = np.zeros_like(self.reward_memory,dtype = np.float64) #For mc Reinforce\n",
    "        for t in range(len(self.reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1 \n",
    "            for k in range(t,len(self.reward_memory)): #why from t? Rewards to go! \n",
    "                G_sum += self.reward_memory[k] * discount\n",
    "                discount *= self.gamma  # decreases for future time steps. \n",
    "                \n",
    "            G[t] = G_sum  # at the end of episode, store sum of returns at timestep t\n",
    "            \n",
    "        #standardize to reduce variance. Free lunch! \n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "\n",
    "        G = (G - mean)/std\n",
    "        G = torch.Tensor(G).to(self.policy.device) #for some reason, worked like this. Phil had to use a specific data type. \n",
    "        loss = 0\n",
    "        for g, logprob in zip(G, self.action_memory):\n",
    "            loss += -g * logprob #weight each probbility by future + current reward at that timestpe\n",
    "\n",
    "                #it's objective is to maximize this prbabil8ty \n",
    "            #spoilers. To do this for multiple trajectories, need to iterate over N episodes, and average these losses. \n",
    "        #backprop!\n",
    "\n",
    "        loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "\n",
    "        #zero out and repeat. This is a Sample inefficient MC, and a future improvement \n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "\n",
    "            #now for the main!\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score  -144.9586949310644\n",
      "episode:  1 score  -271.8127205443703\n",
      "episode:  2 score  -150.34019583109477\n",
      "episode:  3 score  -236.44527665051433\n",
      "episode:  4 score  -135.70245116131235\n",
      "episode:  5 score  -229.28413205842548\n",
      "episode:  6 score  -411.3766901649122\n",
      "episode:  7 score  -260.1035301709883\n",
      "episode:  8 score  -251.99946036992083\n",
      "episode:  9 score  -155.6462985607587\n",
      "episode:  10 score  -158.22565650676142\n",
      "episode:  11 score  -403.976762394633\n",
      "episode:  12 score  -162.04154753061934\n",
      "episode:  13 score  -293.47655596708614\n",
      "episode:  14 score  -153.94195249684353\n",
      "episode:  15 score  -269.3367674337127\n",
      "episode:  16 score  -124.04590077276593\n",
      "episode:  17 score  -235.42601321899934\n",
      "episode:  18 score  -165.2374716140478\n",
      "episode:  19 score  -404.52947895098316\n",
      "episode:  20 score  -110.28587361456435\n",
      "episode:  21 score  -120.53795422042411\n",
      "episode:  22 score  -171.10525550659509\n",
      "episode:  23 score  -89.20117998954778\n",
      "episode:  24 score  -149.89164832593983\n",
      "episode:  25 score  -392.9185580496247\n",
      "episode:  26 score  -122.47916859723809\n",
      "episode:  27 score  -374.68482255249484\n",
      "episode:  28 score  -263.45166669827825\n",
      "episode:  29 score  -199.80846895768175\n",
      "episode:  30 score  -148.4988971766815\n",
      "episode:  31 score  -146.40607393108203\n",
      "episode:  32 score  -82.70576928496016\n",
      "episode:  33 score  -84.72866944257147\n",
      "episode:  34 score  -189.329437513549\n",
      "episode:  35 score  -493.6368240703073\n",
      "episode:  36 score  -268.97060085913506\n",
      "episode:  37 score  -371.8169543182621\n",
      "episode:  38 score  -271.1116467491987\n",
      "episode:  39 score  -138.6402846380863\n",
      "episode:  40 score  -511.765188933997\n",
      "episode:  41 score  -569.3426364683206\n",
      "episode:  42 score  -120.81445807170778\n",
      "episode:  43 score  -581.0813297910306\n",
      "episode:  44 score  -191.30544873131745\n",
      "episode:  45 score  -380.9895505606466\n",
      "episode:  46 score  -125.36635424861004\n",
      "episode:  47 score  -364.9773594207811\n",
      "episode:  48 score  -323.55112871689107\n",
      "episode:  49 score  -88.16343761435748\n",
      "episode:  50 score  -447.57068084780906\n",
      "episode:  51 score  -420.546933691076\n",
      "episode:  52 score  -278.6054623254438\n",
      "episode:  53 score  -237.18013910717062\n",
      "episode:  54 score  -122.865243873544\n",
      "episode:  55 score  -150.51355978471207\n",
      "episode:  56 score  -371.69735250824544\n",
      "episode:  57 score  -249.09925319857564\n",
      "episode:  58 score  -142.13802578198408\n",
      "episode:  59 score  -115.77947735882891\n",
      "episode:  60 score  -254.0184361703994\n",
      "episode:  61 score  -69.85926948387156\n",
      "episode:  62 score  -347.21176392470807\n",
      "episode:  63 score  -325.0038831319173\n",
      "episode:  64 score  -460.99956067677374\n",
      "episode:  65 score  -202.48362214069468\n",
      "episode:  66 score  -449.7394211333689\n",
      "episode:  67 score  -271.99221102939447\n",
      "episode:  68 score  -542.4535637699898\n",
      "episode:  69 score  -40.217200013010775\n",
      "episode:  70 score  -13.406927879642794\n",
      "episode:  71 score  -152.27763651827638\n",
      "episode:  72 score  -133.18856433834407\n",
      "episode:  73 score  -259.25968249433623\n",
      "episode:  74 score  -136.2485185418528\n",
      "episode:  75 score  -112.09274295166946\n",
      "episode:  76 score  -257.76127095177895\n",
      "episode:  77 score  -149.24637624273146\n",
      "episode:  78 score  -51.09005181335745\n",
      "episode:  79 score  -314.801631612855\n",
      "episode:  80 score  -281.54367773349765\n",
      "episode:  81 score  -361.82860091549713\n",
      "episode:  82 score  -515.1656330013495\n",
      "episode:  83 score  -227.65450192131638\n",
      "episode:  84 score  -383.28663107938655\n",
      "episode:  85 score  -279.6889587318766\n",
      "episode:  86 score  -367.3483471802208\n",
      "episode:  87 score  -507.8805032064025\n",
      "episode:  88 score  -745.4301419985103\n",
      "episode:  89 score  -143.01207892978493\n",
      "episode:  90 score  -254.46970244418569\n",
      "episode:  91 score  -350.4608919735917\n",
      "episode:  92 score  -619.1977320923561\n",
      "episode:  93 score  -626.7624379737598\n",
      "episode:  94 score  -209.72122690940606\n",
      "episode:  95 score  -271.9369212562734\n",
      "episode:  96 score  -524.7274984886487\n",
      "episode:  97 score  -559.0306549056631\n",
      "episode:  98 score  -276.77117840999597\n",
      "episode:  99 score  -280.9701571478117\n",
      "episode:  100 score  -218.21745340868267\n",
      "episode:  101 score  -187.41863844527427\n",
      "episode:  102 score  -405.76829171696505\n",
      "episode:  103 score  -143.3389533691157\n",
      "episode:  104 score  -319.71230561081063\n",
      "episode:  105 score  -439.38451764510995\n",
      "episode:  106 score  -111.22828527468052\n",
      "episode:  107 score  -434.8766989802799\n",
      "episode:  108 score  -222.73169584744636\n",
      "episode:  109 score  -274.06301227908045\n",
      "episode:  110 score  -76.14633854979687\n",
      "episode:  111 score  -78.32650792122092\n",
      "episode:  112 score  -238.98020150627386\n",
      "episode:  113 score  -54.89767436544956\n",
      "episode:  114 score  -80.85921672033007\n",
      "episode:  115 score  -121.9618162072963\n",
      "episode:  116 score  -127.92187572951333\n",
      "episode:  117 score  -331.521579268672\n",
      "episode:  118 score  -109.52668729125651\n",
      "episode:  119 score  -294.59459417085986\n",
      "episode:  120 score  -168.24548447762544\n",
      "episode:  121 score  -51.4188779008594\n",
      "episode:  122 score  -123.37907257384714\n",
      "episode:  123 score  -367.6196345639111\n",
      "episode:  124 score  -129.0719196167322\n",
      "episode:  125 score  -127.77197394995169\n",
      "episode:  126 score  -117.61861264636428\n",
      "episode:  127 score  -213.7783191586932\n",
      "episode:  128 score  -64.70973295252828\n",
      "episode:  129 score  -87.39716228752383\n",
      "episode:  130 score  -167.88211225759898\n",
      "episode:  131 score  -332.51994050301795\n",
      "episode:  132 score  -71.86103977313883\n",
      "episode:  133 score  -65.52098849453796\n",
      "episode:  134 score  -81.65726845656457\n",
      "episode:  135 score  -166.90943866454992\n",
      "episode:  136 score  -93.85674359749699\n",
      "episode:  137 score  -90.28842127445398\n",
      "episode:  138 score  -92.78307531571136\n",
      "episode:  139 score  -39.90823548060136\n",
      "episode:  140 score  -121.57707216379717\n",
      "episode:  141 score  -62.78280773278665\n",
      "episode:  142 score  -134.84001942070321\n",
      "episode:  143 score  -255.3879988270351\n",
      "episode:  144 score  -54.28608061669047\n",
      "episode:  145 score  -78.6799170555114\n",
      "episode:  146 score  -92.36738743332253\n",
      "episode:  147 score  -85.01479212584307\n",
      "episode:  148 score  -74.68272226623408\n",
      "episode:  149 score  -40.51703588174411\n",
      "episode:  150 score  -86.58735224400293\n",
      "episode:  151 score  -122.05048834821451\n",
      "episode:  152 score  -94.96122121129713\n",
      "episode:  153 score  -125.25226318154628\n",
      "episode:  154 score  -297.77644312658254\n",
      "episode:  155 score  -416.0401919665693\n",
      "episode:  156 score  -98.66999433852227\n",
      "episode:  157 score  -115.65192480674156\n",
      "episode:  158 score  -342.70310044283303\n",
      "episode:  159 score  -544.1940102998108\n",
      "episode:  160 score  -341.6760293968681\n",
      "episode:  161 score  -117.58857761156722\n",
      "episode:  162 score  -39.46453830772079\n",
      "episode:  163 score  -50.314871284840194\n",
      "episode:  164 score  -443.0275703564256\n",
      "episode:  165 score  -46.86682447338548\n",
      "episode:  166 score  -118.09280144642929\n",
      "episode:  167 score  -110.77458804763626\n",
      "episode:  168 score  -86.87924379104051\n",
      "episode:  169 score  -85.08358152072118\n",
      "episode:  170 score  -105.67132641039234\n",
      "episode:  171 score  -40.97732634718905\n",
      "episode:  172 score  -199.07778167674195\n",
      "episode:  173 score  -53.50669203155\n",
      "episode:  174 score  -110.96271465014408\n",
      "episode:  175 score  -156.4803995502049\n",
      "episode:  176 score  -55.808485054379744\n",
      "episode:  177 score  -33.424185989940014\n",
      "episode:  178 score  -195.82548823232236\n",
      "episode:  179 score  -218.9776475619234\n",
      "episode:  180 score  -14.972841890312068\n",
      "episode:  181 score  -57.74090849286264\n",
      "episode:  182 score  -351.19391487848384\n",
      "episode:  183 score  -249.58831403296443\n",
      "episode:  184 score  -193.45410708774932\n",
      "episode:  185 score  -69.76304937228325\n",
      "episode:  186 score  -83.3298705279989\n",
      "episode:  187 score  -302.96660007624524\n",
      "episode:  188 score  -260.4334885343186\n",
      "episode:  189 score  -86.46868074532632\n",
      "episode:  190 score  -247.9341951057314\n",
      "episode:  191 score  -196.01419953015755\n",
      "episode:  192 score  -430.8465550011873\n",
      "episode:  193 score  -241.28776481458175\n",
      "episode:  194 score  -295.8934092514752\n",
      "episode:  195 score  -400.3168494376675\n",
      "episode:  196 score  -49.947940998977174\n",
      "episode:  197 score  -70.82356974683205\n",
      "episode:  198 score  -87.66654845874629\n",
      "episode:  199 score  -60.1692005845166\n",
      "episode:  200 score  -81.10359734652225\n",
      "episode:  201 score  -72.19854009328346\n",
      "episode:  202 score  -180.42711543885133\n",
      "episode:  203 score  -15.34554637385638\n",
      "episode:  204 score  -22.925940786886514\n",
      "episode:  205 score  -37.45538249494575\n",
      "episode:  206 score  -23.861495061924003\n",
      "episode:  207 score  -101.53808758643973\n",
      "episode:  208 score  -95.45573443372015\n",
      "episode:  209 score  -69.06128990614798\n",
      "episode:  210 score  -57.02959548003997\n",
      "episode:  211 score  -63.69642242473368\n",
      "episode:  212 score  -92.48396288867578\n",
      "episode:  213 score  -20.270933093516675\n",
      "episode:  214 score  -59.28928778112724\n",
      "episode:  215 score  -50.35462615356727\n",
      "episode:  216 score  -143.26174970313576\n",
      "episode:  217 score  -51.967215380105124\n",
      "episode:  218 score  -81.83860084714719\n",
      "episode:  219 score  -109.58778820100764\n",
      "episode:  220 score  -18.04443302564178\n",
      "episode:  221 score  -39.4893190970772\n",
      "episode:  222 score  1.4242409471373492\n",
      "episode:  223 score  5.495623963607512\n",
      "episode:  224 score  -86.56843272180407\n",
      "episode:  225 score  -3.9555627937314313\n",
      "episode:  226 score  -87.14817671062005\n",
      "episode:  227 score  -96.12985444478777\n",
      "episode:  228 score  -112.09545883148145\n",
      "episode:  229 score  -281.35519837623707\n",
      "episode:  230 score  -27.670088251351718\n",
      "episode:  231 score  -61.65401029011454\n",
      "episode:  232 score  -15.633185118372836\n",
      "episode:  233 score  -65.68633669254194\n",
      "episode:  234 score  -16.999815762278285\n",
      "episode:  235 score  -255.57574508025888\n",
      "episode:  236 score  -189.72854326359212\n",
      "episode:  237 score  -199.9009784705496\n",
      "episode:  238 score  -378.049424091834\n",
      "episode:  239 score  -74.65406467197398\n",
      "episode:  240 score  -37.873686208232584\n",
      "episode:  241 score  -68.3830517913646\n",
      "episode:  242 score  -111.3991551780118\n",
      "episode:  243 score  -95.64084397939189\n",
      "episode:  244 score  -39.99473603738343\n",
      "episode:  245 score  -28.093217406049803\n",
      "episode:  246 score  -192.14988459918186\n",
      "episode:  247 score  -36.80564868748053\n",
      "episode:  248 score  -49.49248618245876\n",
      "episode:  249 score  -111.64248950344478\n",
      "episode:  250 score  -2.6132288103033545\n",
      "episode:  251 score  -86.91373341178961\n",
      "episode:  252 score  -192.1736623908846\n",
      "episode:  253 score  -52.8264923234432\n",
      "episode:  254 score  -18.969127584263262\n",
      "episode:  255 score  -34.799019226468616\n",
      "episode:  256 score  -68.4950799436024\n",
      "episode:  257 score  -82.10432911651425\n",
      "episode:  258 score  -215.22696540083237\n",
      "episode:  259 score  -273.344559561324\n",
      "episode:  260 score  9.126739512852524\n",
      "episode:  261 score  -68.20774140414444\n",
      "episode:  262 score  -84.36279639617773\n",
      "episode:  263 score  -46.26481549926339\n",
      "episode:  264 score  -208.99030891881551\n",
      "episode:  265 score  -36.57814926220746\n",
      "episode:  266 score  -548.3267462548406\n",
      "episode:  267 score  -84.28879451897956\n",
      "episode:  268 score  -43.3796832605899\n",
      "episode:  269 score  -169.13846617874634\n",
      "episode:  270 score  20.41988016961656\n",
      "episode:  271 score  -25.260363475025457\n",
      "episode:  272 score  -90.3157145213991\n",
      "episode:  273 score  -64.32873414778491\n",
      "episode:  274 score  -76.42570388000752\n",
      "episode:  275 score  -9.64297882256568\n",
      "episode:  276 score  -35.22506090761145\n",
      "episode:  277 score  -28.65721315142376\n",
      "episode:  278 score  -41.438583005084624\n",
      "episode:  279 score  -31.13007442775877\n",
      "episode:  280 score  -327.16593174189325\n",
      "episode:  281 score  -63.097211730937346\n",
      "episode:  282 score  -51.400035107992906\n",
      "episode:  283 score  -7.152367459299171\n",
      "episode:  284 score  -54.98034246336685\n",
      "episode:  285 score  -37.54688140095528\n",
      "episode:  286 score  -59.34140716629635\n",
      "episode:  287 score  -74.24266475343875\n",
      "episode:  288 score  -91.03626324368878\n",
      "episode:  289 score  -100.58983488173942\n",
      "episode:  290 score  -106.85503159379502\n",
      "episode:  291 score  -44.75544995730998\n",
      "episode:  292 score  -34.029327993106335\n",
      "episode:  293 score  -66.38139470609235\n",
      "episode:  294 score  -107.84593644603353\n",
      "episode:  295 score  -38.516085716064644\n",
      "episode:  296 score  -15.22430911154845\n",
      "episode:  297 score  -41.65566301047562\n",
      "episode:  298 score  -43.13070563953792\n",
      "episode:  299 score  -47.37896625070058\n",
      "episode:  300 score  -56.49527757182638\n",
      "episode:  301 score  -22.273824195132335\n",
      "episode:  302 score  -149.271924224012\n",
      "episode:  303 score  3.2261068630631087\n",
      "episode:  304 score  -67.40654677016221\n",
      "episode:  305 score  -127.3456522019058\n",
      "episode:  306 score  -73.57750872431555\n",
      "episode:  307 score  -22.160065532967863\n",
      "episode:  308 score  -180.2693797952662\n",
      "episode:  309 score  -52.282607660203965\n",
      "episode:  310 score  -43.7087351045147\n",
      "episode:  311 score  -19.682134336763642\n",
      "episode:  312 score  8.977463578090262\n",
      "episode:  313 score  -83.6356322685612\n",
      "episode:  314 score  -94.5006529083563\n",
      "episode:  315 score  -75.06125612757226\n",
      "episode:  316 score  -140.7316109856887\n",
      "episode:  317 score  -278.38574063550925\n",
      "episode:  318 score  -3.6998496491039488\n",
      "episode:  319 score  -222.7445390648861\n",
      "episode:  320 score  -102.36512740122494\n",
      "episode:  321 score  -351.4030705773562\n",
      "episode:  322 score  -56.45589014877032\n",
      "episode:  323 score  -103.82078948831474\n",
      "episode:  324 score  -127.73578466157532\n",
      "episode:  325 score  -13.52031867287409\n",
      "episode:  326 score  -29.294154239930144\n",
      "episode:  327 score  31.67956112035775\n",
      "episode:  328 score  -41.19084390696754\n",
      "episode:  329 score  -225.54764964647046\n",
      "episode:  330 score  -52.90321322910765\n",
      "episode:  331 score  -11.646861864957458\n",
      "episode:  332 score  -48.79743407572481\n",
      "episode:  333 score  -42.53673631464399\n",
      "episode:  334 score  -56.12729433763194\n",
      "episode:  335 score  10.41177664658223\n",
      "episode:  336 score  -4.847236211513659\n",
      "episode:  337 score  -71.21271457871975\n",
      "episode:  338 score  -51.54724363529398\n",
      "episode:  339 score  -19.45515119755828\n",
      "episode:  340 score  -124.44360145956017\n",
      "episode:  341 score  -92.31034917746699\n",
      "episode:  342 score  3.4678067969296684\n",
      "episode:  343 score  -144.16043002150224\n",
      "episode:  344 score  -33.400566624669395\n",
      "episode:  345 score  -200.359221013338\n",
      "episode:  346 score  -43.485974338029926\n",
      "episode:  347 score  -163.12732219061115\n",
      "episode:  348 score  -119.6076026794496\n",
      "episode:  349 score  -259.85904350177583\n",
      "episode:  350 score  -244.8457004967375\n",
      "episode:  351 score  -269.9462267035559\n",
      "episode:  352 score  -337.488912844272\n",
      "episode:  353 score  -205.22985889430726\n",
      "episode:  354 score  -172.34107304741823\n",
      "episode:  355 score  -117.46711254973962\n",
      "episode:  356 score  -91.77427671840455\n",
      "episode:  357 score  -138.90318717154958\n",
      "episode:  358 score  -291.53293198197525\n",
      "episode:  359 score  -95.37515365999258\n",
      "episode:  360 score  -170.9909786020125\n",
      "episode:  361 score  -167.23135480000445\n",
      "episode:  362 score  -146.18935304041707\n",
      "episode:  363 score  -367.8997692440115\n",
      "episode:  364 score  -246.97942107187964\n",
      "episode:  365 score  -266.4088925052698\n",
      "episode:  366 score  -415.294777268483\n",
      "episode:  367 score  -267.58324959155146\n",
      "episode:  368 score  -293.29089357077464\n",
      "episode:  369 score  -161.21758959828742\n",
      "episode:  370 score  -261.66322547828554\n",
      "episode:  371 score  -317.7574868305141\n",
      "episode:  372 score  -280.20801425862714\n",
      "episode:  373 score  -247.18515595435431\n",
      "episode:  374 score  -100.02101203726603\n",
      "episode:  375 score  -283.63570040821344\n",
      "episode:  376 score  -376.0365663238634\n",
      "episode:  377 score  -32.864158670807385\n",
      "episode:  378 score  -116.61838901085137\n",
      "episode:  379 score  -47.280127901307075\n",
      "episode:  380 score  -288.1528203319558\n",
      "episode:  381 score  -76.95458710635025\n",
      "episode:  382 score  -100.15482675724259\n",
      "episode:  383 score  -39.54578864997079\n"
     ]
    }
   ],
   "source": [
    "#Main \n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(lr = 0.001, input_dims=[8],gamma=0.99,n_actions=4\n",
    "             ,l1_size=128,l2_size=128)\n",
    "\n",
    "score_history = []\n",
    "score = 0\n",
    "n_episodes = 2500\n",
    "\n",
    "#env = wrappers.Monitor(env, 'tmp/lunar-lander',\n",
    " #                      video_callable=lambda episodeid: True, force=True)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_,reward, done, info = env.step(action)\n",
    "        agent.store_rewards(reward)\n",
    "        observation = observation_ #set the old obvs to the new one\n",
    "        score += reward\n",
    "        \n",
    "    score_history.append(score)\n",
    "    agent.learn()\n",
    "    print('episode: ', i, 'score ', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
