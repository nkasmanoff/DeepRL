{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Implementation of Deep Q Network from https://www.youtube.com/watch?v=UlJzzLYgYoE&t=3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN has several sub-modules: Experience Buffer, the network, and the agent. Bringing all these together makes the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, ALPHA, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=ALPHA)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        state = T.Tensor(observation).to(self.device)\n",
    "        #observation = observation.view(-1)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        return actions\n",
    "\n",
    "class Agent(object):\n",
    "    #No target update? \n",
    "    def __init__(self, gamma, epsilon, alpha, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=0.996):\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.EPS_MIN = eps_end\n",
    "        self.EPS_DEC = eps_dec\n",
    "        self.ALPHA = alpha\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.n_actions = n_actions\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.Q_eval = DeepQNetwork(alpha, n_actions=self.n_actions,\n",
    "                              input_dims=input_dims, fc1_dims=256, fc2_dims=256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims))\n",
    "        self.action_memory = np.zeros((self.mem_size, self.n_actions),\n",
    "                                      dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "\n",
    "    def storeTransition(self, state, action, reward, state_, terminal):\n",
    "        \n",
    "        #stores s a s' r and done flag for td updates. \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        actions = np.zeros(self.n_actions)\n",
    "        actions[action] = 1.0 #one hot encodes the action\n",
    "        self.action_memory[index] = actions\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = 1 - terminal\n",
    "        self.mem_cntr += 1 #update total size so we know when to update it. \n",
    "\n",
    "    def chooseAction(self, observation):\n",
    "        #select an action according to epsilon greedy strategy. \n",
    "        rand = np.random.random()\n",
    "        actions = self.Q_eval.forward(observation)\n",
    "        if rand > self.EPSILON:\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr > self.batch_size: \n",
    "            #once you have accumulated enough transitions, begin training. \n",
    "            self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "            max_mem = self.mem_cntr if self.mem_cntr < self.mem_size \\\n",
    "                                    else self.mem_size\n",
    "\n",
    "            batch = np.random.choice(max_mem, self.batch_size)\n",
    "            state_batch = self.state_memory[batch]\n",
    "            action_batch = self.action_memory[batch]\n",
    "            action_values = np.array(self.action_space, dtype=np.int32)\n",
    "            action_indices = np.dot(action_batch, action_values)\n",
    "            reward_batch = self.reward_memory[batch]\n",
    "            new_state_batch = self.new_state_memory[batch]\n",
    "            terminal_batch = self.terminal_memory[batch]\n",
    "\n",
    "            reward_batch = T.Tensor(reward_batch).to(self.Q_eval.device)\n",
    "            terminal_batch = T.Tensor(terminal_batch).to(self.Q_eval.device)\n",
    "\n",
    "            q_eval = self.Q_eval.forward(state_batch).to(self.Q_eval.device)\n",
    "            #q_target = self.Q_eval.forward(state_batch).to(self.Q_eval.device)\n",
    "            q_target = q_eval.clone()\n",
    "            q_next = self.Q_eval.forward(new_state_batch).to(self.Q_eval.device)\n",
    "\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            q_target[batch_index, action_indices] = reward_batch + \\\n",
    "                                self.GAMMA*T.max(q_next, dim=1)[0]*terminal_batch\n",
    "\n",
    "            self.EPSILON = self.EPSILON*self.EPS_DEC if self.EPSILON > \\\n",
    "                           self.EPS_MIN else self.EPS_MIN\n",
    "\n",
    "            loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "            loss.backward()\n",
    "            self.Q_eval.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode i 1 average score -250.5051402672164\n",
      "Episode i 2 average score -217.75465659867598\n",
      "Episode i 3 average score -336.50625671976104\n",
      "Episode i 4 average score -366.23928664406805\n",
      "Episode i 5 average score -317.5573775974397\n",
      "Episode i 6 average score -301.2327110128817\n",
      "Episode i 7 average score -281.2038357501346\n",
      "Episode i 8 average score -294.8833911869876\n",
      "Episode i 9 average score -272.53824063169344\n",
      "Episode i 11 average score -266.86297202610893\n",
      "Episode i 12 average score -255.6515049377659\n",
      "Episode i 13 average score -206.82875224866197\n",
      "Episode i 14 average score -168.61399885819316\n",
      "Episode i 15 average score -187.83796444387772\n",
      "Episode i 16 average score -177.1692048409735\n",
      "Episode i 17 average score -168.92852237707302\n",
      "Episode i 18 average score -129.8473481618523\n",
      "Episode i 19 average score -118.83303264291408\n",
      "Episode i 21 average score -69.95531758822594\n",
      "Episode i 22 average score -54.8435552081358\n",
      "Episode i 23 average score -30.456783497587487\n",
      "Episode i 24 average score -8.645493981722296\n",
      "Episode i 25 average score 34.701950764160905\n",
      "Episode i 26 average score 50.84862318531732\n",
      "Episode i 27 average score 56.65126416857163\n",
      "Episode i 28 average score 42.66912494408217\n",
      "Episode i 29 average score 58.6835574621054\n",
      "Episode i 31 average score 13.586865660188584\n",
      "Episode i 32 average score 25.632831179887887\n",
      "Episode i 33 average score -9.830803988391361\n",
      "Episode i 34 average score -11.665487497050657\n",
      "Episode i 35 average score -1.660737621762388\n",
      "Episode i 36 average score -15.175681834183653\n",
      "Episode i 37 average score -46.18172061043269\n",
      "Episode i 38 average score -18.65238951218048\n",
      "Episode i 39 average score -25.655325487177798\n",
      "Episode i 41 average score 6.797927169890931\n",
      "Episode i 42 average score -28.218721897032527\n",
      "Episode i 43 average score 10.09520043250364\n",
      "Episode i 44 average score 11.176576841642227\n",
      "Episode i 45 average score 9.175527613659336\n",
      "Episode i 46 average score 36.16281048646749\n",
      "Episode i 47 average score 63.45039750610861\n",
      "Episode i 48 average score 71.54428767501709\n",
      "Episode i 49 average score 73.96187642449956\n",
      "Episode i 51 average score 121.17760530768842\n",
      "Episode i 52 average score 112.98929114119804\n",
      "Episode i 53 average score 116.0102515887294\n",
      "Episode i 54 average score 98.71909619350546\n",
      "Episode i 55 average score 65.32694789034454\n",
      "Episode i 56 average score 65.65162549067489\n",
      "Episode i 57 average score 63.338263557116264\n",
      "Episode i 58 average score 58.18459099557528\n",
      "Episode i 59 average score 61.976472528371936\n",
      "Episode i 61 average score 62.19716623744516\n",
      "Episode i 62 average score 101.89561702057733\n",
      "Episode i 63 average score 97.09469826876169\n",
      "Episode i 64 average score 122.31618550733724\n",
      "Episode i 65 average score 155.5808746548462\n",
      "Episode i 66 average score 150.84152064782563\n",
      "Episode i 67 average score 178.96653298755388\n",
      "Episode i 68 average score 157.77600614691454\n",
      "Episode i 69 average score 153.6470952605831\n",
      "Episode i 71 average score 151.03572113647073\n",
      "Episode i 72 average score 156.35521067742704\n",
      "Episode i 73 average score 161.42366711938536\n",
      "Episode i 74 average score 161.7018819524072\n",
      "Episode i 75 average score 165.94041612773555\n",
      "Episode i 76 average score 168.23901081619877\n",
      "Episode i 77 average score 168.86667556066553\n",
      "Episode i 78 average score 198.03915301179165\n",
      "Episode i 79 average score 205.53607785515032\n",
      "Episode i 81 average score 190.60072172438845\n",
      "Episode i 82 average score 160.9613577149408\n",
      "Episode i 83 average score 159.8781008927046\n",
      "Episode i 84 average score 161.73888761573178\n",
      "Episode i 85 average score 125.54708214274467\n",
      "Episode i 86 average score 135.40125050663536\n",
      "Episode i 87 average score 105.83723740680682\n",
      "Episode i 88 average score 80.18602542941558\n",
      "Episode i 89 average score 45.2172859723811\n",
      "Episode i 91 average score 62.18557546356858\n",
      "Episode i 92 average score 90.01659950878941\n",
      "Episode i 93 average score 89.0830208790361\n",
      "Episode i 94 average score 81.13800901509916\n",
      "Episode i 95 average score 111.48839788863066\n",
      "Episode i 96 average score 63.18445458589444\n",
      "Episode i 97 average score 93.35022234178052\n",
      "Episode i 98 average score 79.85332241310373\n",
      "Episode i 99 average score 89.68225179691593\n",
      "Episode i 101 average score 98.67288402606322\n",
      "Episode i 102 average score 70.42740304424397\n",
      "Episode i 103 average score 67.4078808965346\n",
      "Episode i 104 average score 71.33137466819574\n",
      "Episode i 105 average score 70.60679465432415\n",
      "Episode i 106 average score 92.4376982217215\n",
      "Episode i 107 average score 90.42675496932841\n",
      "Episode i 108 average score 84.70741127507637\n",
      "Episode i 109 average score 76.82202329582525\n",
      "Episode i 111 average score 76.14855080616356\n",
      "Episode i 112 average score 63.78513456891121\n",
      "Episode i 113 average score 69.32802359376717\n",
      "Episode i 114 average score 67.6540335623308\n",
      "Episode i 115 average score 42.10071660252037\n",
      "Episode i 116 average score 59.43177824697047\n",
      "Episode i 117 average score 63.92374309810327\n",
      "Episode i 118 average score 107.24102962842099\n",
      "Episode i 119 average score 139.7013802770212\n",
      "Episode i 121 average score 80.39664637722493\n",
      "Episode i 122 average score 121.42822667601988\n",
      "Episode i 123 average score 120.58286774644912\n",
      "Episode i 124 average score 83.871864685217\n",
      "Episode i 125 average score 112.85350277488457\n",
      "Episode i 126 average score 117.77089849440063\n",
      "Episode i 127 average score 109.77339447665513\n",
      "Episode i 128 average score 104.86559895325342\n",
      "Episode i 129 average score 104.73311577943487\n",
      "Episode i 131 average score 160.38068726274338\n",
      "Episode i 132 average score 161.70982503090897\n",
      "Episode i 133 average score 164.2669780334642\n",
      "Episode i 134 average score 174.93974753161174\n",
      "Episode i 135 average score 178.06141094585578\n",
      "Episode i 136 average score 147.3967147010287\n",
      "Episode i 137 average score 151.68179372709338\n",
      "Episode i 138 average score 130.40188638011688\n",
      "Episode i 139 average score 129.13427958790072\n",
      "Episode i 141 average score 130.48327994488477\n",
      "Episode i 142 average score 127.25116693293653\n",
      "Episode i 143 average score 74.21233860677665\n",
      "Episode i 144 average score 102.38069793443506\n",
      "Episode i 145 average score 78.02771348232076\n",
      "Episode i 146 average score 105.31259296752282\n",
      "Episode i 147 average score 74.49044629105106\n",
      "Episode i 148 average score 96.2321422327699\n",
      "Episode i 149 average score 91.16099786964978\n",
      "Episode i 151 average score 67.27873470549117\n",
      "Episode i 152 average score 71.2307490968886\n",
      "Episode i 153 average score 123.38335788065099\n",
      "Episode i 154 average score 119.53645241930826\n",
      "Episode i 155 average score 143.02691951081428\n",
      "Episode i 156 average score 121.19213953562478\n",
      "Episode i 157 average score 153.2571374076015\n",
      "Episode i 158 average score 154.98416315358494\n",
      "Episode i 159 average score 130.3539069637723\n",
      "Episode i 161 average score 157.95215179853963\n",
      "Episode i 162 average score 156.22983385271382\n",
      "Episode i 163 average score 157.74035464791692\n",
      "Episode i 164 average score 161.73388665330995\n",
      "Episode i 165 average score 161.5635389968674\n",
      "Episode i 166 average score 184.17830570933012\n",
      "Episode i 167 average score 183.06078609849473\n",
      "Episode i 168 average score 185.44559405285221\n",
      "Episode i 169 average score 219.91428460671528\n",
      "Episode i 171 average score 194.05133224270497\n",
      "Episode i 172 average score 198.64583965308935\n",
      "Episode i 173 average score 196.33838519135472\n",
      "Episode i 174 average score 196.5072320635259\n",
      "Episode i 175 average score 195.56219371341166\n",
      "Episode i 176 average score 197.4957341282139\n",
      "Episode i 177 average score 197.1053570710434\n",
      "Episode i 178 average score 194.4758615955003\n",
      "Episode i 179 average score 191.65924099706294\n",
      "Episode i 181 average score 189.86583817666457\n",
      "Episode i 182 average score 164.20654380399907\n",
      "Episode i 183 average score 162.29690321141427\n",
      "Episode i 184 average score 131.5108280729307\n",
      "Episode i 185 average score 104.46990043416545\n",
      "Episode i 186 average score 105.00444379034124\n",
      "Episode i 187 average score 106.53137369782046\n",
      "Episode i 188 average score 103.90237154293838\n",
      "Episode i 189 average score 79.89544357802643\n",
      "Episode i 191 average score 98.25867652872773\n",
      "Episode i 192 average score 117.61989773015878\n",
      "Episode i 193 average score 114.93353451321325\n",
      "Episode i 194 average score 150.00571621274378\n",
      "Episode i 195 average score 177.57697396712678\n",
      "Episode i 196 average score 178.34134257771842\n",
      "Episode i 197 average score 98.66834426749567\n",
      "Episode i 198 average score 102.8762708950031\n",
      "Episode i 199 average score 104.8486576381868\n",
      "Episode i 201 average score 85.4749700052492\n",
      "Episode i 202 average score 88.2919705964885\n",
      "Episode i 203 average score 90.74791544287216\n",
      "Episode i 204 average score 52.16949518263398\n",
      "Episode i 205 average score 53.321751383799665\n",
      "Episode i 206 average score 47.71476561126134\n",
      "Episode i 207 average score 127.45874059470812\n",
      "Episode i 208 average score 95.42830544350457\n",
      "Episode i 209 average score 117.60561864581419\n",
      "Episode i 211 average score 91.11659741182976\n",
      "Episode i 212 average score 58.20502710084236\n",
      "Episode i 213 average score 56.27307011064524\n",
      "Episode i 214 average score 86.67928003378347\n",
      "Episode i 215 average score 51.35829613565288\n",
      "Episode i 216 average score 55.34510242607265\n",
      "Episode i 217 average score 46.454983788166246\n",
      "Episode i 218 average score 41.51595425122868\n",
      "Episode i 219 average score 40.09302824916632\n",
      "Episode i 221 average score 25.808608813627767\n",
      "Episode i 222 average score 61.407680555505216\n",
      "Episode i 223 average score 34.091008648790854\n",
      "Episode i 224 average score 37.32124406190968\n",
      "Episode i 225 average score 68.1256281868444\n",
      "Episode i 226 average score 69.92586404051474\n",
      "Episode i 227 average score 79.06938335693376\n",
      "Episode i 228 average score 87.24940583418892\n",
      "Episode i 229 average score 87.50633945015056\n",
      "Episode i 231 average score 156.55845234140185\n",
      "Episode i 232 average score 154.8177234875909\n",
      "Episode i 233 average score 182.34550633092326\n",
      "Episode i 234 average score 181.46445816702843\n",
      "Episode i 235 average score 183.18644985087576\n",
      "Episode i 236 average score 181.87044012869936\n",
      "Episode i 237 average score 179.773317022983\n",
      "Episode i 238 average score 209.457267158447\n",
      "Episode i 239 average score 205.56063839055668\n",
      "Episode i 241 average score 205.62824391317199\n",
      "Episode i 242 average score 204.70315291849155\n",
      "Episode i 243 average score 175.7259899558538\n",
      "Episode i 244 average score 175.8320947443097\n",
      "Episode i 245 average score 132.65265669074645\n",
      "Episode i 246 average score 130.6321272542722\n",
      "Episode i 247 average score 132.63845948378466\n",
      "Episode i 248 average score 98.15243305833421\n",
      "Episode i 249 average score 107.46166243916744\n",
      "Episode i 251 average score 99.66652193133699\n",
      "Episode i 252 average score 75.22868770986528\n",
      "Episode i 253 average score 106.5596460216173\n",
      "Episode i 254 average score 104.23011743972059\n",
      "Episode i 255 average score 124.68514759022881\n",
      "Episode i 256 average score 123.12417452590209\n",
      "Episode i 257 average score 68.8115681417247\n",
      "Episode i 258 average score 102.85363840080262\n",
      "Episode i 259 average score 98.63645425657128\n",
      "Episode i 261 average score 106.82679718874456\n",
      "Episode i 262 average score 120.11051016136226\n",
      "Episode i 263 average score 94.07742583337945\n",
      "Episode i 264 average score 86.81514105648208\n",
      "Episode i 265 average score 75.81319889205216\n",
      "Episode i 266 average score 82.90782187118796\n",
      "Episode i 267 average score 77.39687178510678\n",
      "Episode i 268 average score 55.68824954281657\n",
      "Episode i 269 average score 25.717420800402977\n",
      "Episode i 271 average score 25.762621274595443\n",
      "Episode i 272 average score 34.50346870545202\n",
      "Episode i 273 average score 59.0476509164135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b5214ef39c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchooseAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now the main!\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(gamma=0.99,epsilon=1.0,batch_size=64,n_actions=4,input_dims=[8]\n",
    "             ,alpha= 0.003)\n",
    "scores = []\n",
    "eps_history = []\n",
    "n_games = 500\n",
    "score = 0\n",
    "for i in range(n_games):\n",
    "    if i % 10 and i > 0:\n",
    "        avg_score = np.mean(scores[max(0,i-10):(i+1)])\n",
    "        print(\"Episode i\", i, 'average score', avg_score)\n",
    "        \n",
    "    score = 0 \n",
    "    eps_history.append(agent.EPSILON)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.chooseAction(observation)\n",
    "        observation_,reward,done, _ = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        agent.storeTransition(observation,action,reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "\n",
    "    scores.append(score)\n",
    "x = [i+1 for i in range(n_games)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but it looks like the target Q values are continuously "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
