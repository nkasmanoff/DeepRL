{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nkasmanoff/DeepRL/blob/master/Load_CarRacing_In_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prYhQX_dTzVC"
   },
   "source": [
    "This notebook is used to train a DQN on the CarRacing Open AI gym environment, using a GPU, but most importantly, is able to load this environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oow0rc2iaDZ4"
   },
   "source": [
    "# CoLab Preambles\n",
    "\n",
    "Most of the requirements of python packages are already fulfilled on CoLab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes.\n",
    "\n",
    "[](To be done next time: )\n",
    "[](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "7wY4qZhPXotR",
    "outputId": "cdcd2771-3393-4b18-d018-93cb94ed0d36"
   },
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!apt-get install python-opengl -y\n",
    "#!apt install xvfb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "3U99_zgNCk3t",
    "outputId": "27a0bba7-50e0-49b0-db5c-108d6a02219c"
   },
   "outputs": [],
   "source": [
    "#!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giqCoXRwaaH3"
   },
   "source": [
    "For rendering environment, you can use pyvirtualdisplay. So fulfill that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "yrxgO5S4XxI5",
    "outputId": "be441d9c-8bd9-4a2c-ed05-805f96e31beb"
   },
   "outputs": [],
   "source": [
    "#!pip install pyvirtualdisplay\n",
    "#!pip install piglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUSaUTcgat3F"
   },
   "source": [
    "To activate virtual display we need to run a script once for training an agent, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Pn1IAnsDYK4V",
    "outputId": "e6e0a47b-048c-4ab9-f31c-3217482df538"
   },
   "outputs": [],
   "source": [
    "#from pyvirtualdisplay import Display\n",
    "#display = Display(visible=0, size=(1400, 900))\n",
    "#display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AutUqpSRYN1W"
   },
   "outputs": [],
   "source": [
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Jh10T5veI1zk"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JWNVK4NUJUCl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "_H4RZv88Oy57",
    "outputId": "d48b60ce-e78d-4b7a-fe36-2f7feba27a3a"
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "#print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzL1lHWpPQ-_"
   },
   "source": [
    "## Add your own environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "ZGn9Aj9sa4Vk",
    "outputId": "70aa8e6e-5f0c-4875-8a93-002803196eb5"
   },
   "outputs": [],
   "source": [
    "#!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsUglhhic_oE"
   },
   "source": [
    "## Box2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGuvFaoNcoBF"
   },
   "source": [
    "Box2d is a 2D physics engine. You can install it via  and then get started as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LCSdSokZcztQ",
    "outputId": "edeafcb4-bc3d-4cf3-849b-8364cb1a9431"
   },
   "outputs": [],
   "source": [
    "#!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "vmKsxcS7cjzZ",
    "outputId": "2fe50b8f-0e1f-41ec-99eb-20414dd31ab6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1008..1266 -> 258-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 985..1242 -> 257-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11cb3ce50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Box2d Environment\n",
    "env = gym.make('CarRacing-v0')\n",
    "env.reset()\n",
    "#plt.imshow(env.render('rgb_array'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbox5wS7lcw_"
   },
   "source": [
    "# Environment successfully loaded in. \n",
    "\n",
    "# Next up, instantiate DQN Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pvIjZCFdk23X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess(state):\n",
    "    \"\"\"\n",
    "    Same as in HW1. Use these preprocessing functions to \n",
    "    \"\"\"\n",
    "    state = state[:-12:,6:-6]\n",
    "    state = rgb2gray(state)\n",
    "    state = state / 255\n",
    "    state = state  * 2 - 1\n",
    "    \n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def grab_test_history(state,state_history,history_length): \n",
    "    if len(state_history) >= history_length:\n",
    "        state_input = np.array(state_history[-history_length:])\n",
    "\n",
    "    \n",
    "    else:\n",
    "        remainder = history_length - len(state_history)\n",
    "        padding = np.zeros(shape = (remainder,84,84))\n",
    "        state_input = np.array(state_history[-history_length:])\n",
    "        state_input = np.concatenate((padding,state_input))\n",
    "        \n",
    "    state_input = torch.from_numpy(state_input).resize(1,history_length,84,84).float()\n",
    "    return state_input    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q Network module. \n",
    "    \n",
    "    It instantiates the class optimizer, device, and all the layers used. \n",
    "    \n",
    "    Same architecture as the one used in HW1, since that seemed to learn things!\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,lr,in_ch=3,action_dim=3,ch=2):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_ch,out_channels=ch*8,kernel_size=7)\n",
    "        self.conv2 = nn.Conv2d(in_channels=ch*8,out_channels=ch*16,kernel_size=3,stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=ch*16,out_channels=ch*16,kernel_size=7)\n",
    "        self.conv4 = nn.Conv2d(in_channels=ch*16,out_channels=ch*32,kernel_size=3,stride=2)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ch*32,out_channels=ch*32,kernel_size=7)\n",
    "        self.conv6 = nn.Conv2d(in_channels=ch*32,out_channels=ch*64,kernel_size=3,stride=2)\n",
    "        self.fc1 = nn.Linear(64 * ch * 4 * 4,256)\n",
    "        self.fc2 = nn.Linear(256,action_dim)\n",
    "        \n",
    "        self.ch = ch\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        x = x.view(-1, 64 * self.ch * 4 * 4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "     \n",
    "        x[:,0] = torch.tanh(x[:,0])\n",
    "        x[:,1] = torch.sigmoid(x[:,1])\n",
    "        x[:,2] = torch.sigmoid(x[:,2])\n",
    "\n",
    "\n",
    "        return x    \n",
    "    \n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    Agent class, holding the RL hyper-params not necessarily contained in the DQN. \n",
    "    \n",
    "    TODO: Polyak Averaging\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, epsilon, alpha,history_length, batch_size,ch=4, n_actions=3,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=0.996,target_update=100):\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.EPS_MIN = eps_end\n",
    "        self.EPS_DEC = eps_dec\n",
    "        self.alpha = alpha\n",
    "        self.n_actions = n_actions\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.history_length = history_length\n",
    "        self.ch = ch\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr = self.alpha,in_ch=self.history_length,action_dim=self.n_actions,ch=self.ch)\n",
    "                                     \n",
    "        self.Q_target = DeepQNetwork(lr = self.alpha,in_ch=self.history_length,action_dim=self.n_actions,ch=self.ch)\n",
    "                                     \n",
    "        input_dims=[history_length,84,84]\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims))\n",
    "        self.action_memory = np.zeros((self.mem_size, self.n_actions),\n",
    "                                      dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "        self.target_counter = 0 #update target counter \n",
    "        self.target_update = target_update\n",
    "        \n",
    "    def storeTransition(self, state, action, reward, state_, terminal):\n",
    "        \"\"\"\n",
    "        \n",
    "        stores s a s' r and done flag for td updates done with the target network. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        actions = np.zeros(self.n_actions)\n",
    "        #actions[action] = 1.0 #one hot encodes the action\n",
    "        self.action_memory[index] = actions\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = 1 - terminal\n",
    "        self.mem_cntr += 1 #update total size so we know when to update it. \n",
    "\n",
    "    def chooseAction(self, observation):\n",
    "        #select an action according to epsilon greedy strategy. \n",
    "        rand = np.random.random()\n",
    "        action = self.Q_eval.forward(observation)\n",
    "        if rand > self.EPSILON:\n",
    "            action = action.numpy() #\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr > self.batch_size: \n",
    "            #once you have accumulated enough transitions, begin training. \n",
    "            self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "            max_mem = self.mem_cntr if self.mem_cntr < self.mem_size \\\n",
    "                                    else self.mem_size\n",
    "\n",
    "            \n",
    "            batch = np.random.choice(max_mem, self.batch_size)\n",
    "            \n",
    "            state_batch = self.state_memory[batch]\n",
    "            action_batch = self.action_memory[batch]\n",
    "            action_values = np.array(self.n_actions, dtype=np.int32)\n",
    "            action_indices = np.dot(action_batch, action_values)\n",
    "            reward_batch = self.reward_memory[batch]\n",
    "            new_state_batch = self.new_state_memory[batch]\n",
    "            terminal_batch = self.terminal_memory[batch]\n",
    "\n",
    "            reward_batch = torch.Tensor(reward_batch).to(self.Q_eval.device)\n",
    "            terminal_batch = torch.Tensor(terminal_batch).to(self.Q_eval.device)\n",
    "\n",
    "            q_eval = self.Q_eval.forward(torch.from_numpy(state_batch).float()).to(self.Q_eval.device)\n",
    "            #q_target = self.Q_eval.forward(state_batch).to(self.Q_eval.device)\n",
    "            q_target = self.Q_target.forward(torch.from_numpy(state_batch).float()).to(self.Q_eval.device)\n",
    "            q_next = self.Q_eval.forward(torch.from_numpy(new_state_batch).float()).to(self.Q_eval.device)\n",
    "\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            q_target[batch_index, action_indices] = reward_batch + \\\n",
    "                                self.GAMMA*torch.max(q_next, dim=1)[0]*terminal_batch\n",
    "\n",
    "            self.EPSILON = self.EPSILON*self.EPS_DEC if self.EPSILON > \\\n",
    "                           self.EPS_MIN else self.EPS_MIN\n",
    "\n",
    "            loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "            loss.backward()\n",
    "            self.Q_eval.optimizer.step()\n",
    "            self.target_counter += 1\n",
    "            \n",
    "            if self.target_counter >= self.target_update:\n",
    "                #update weights, reset counter\n",
    "                self.Q_target.load_state_dict(self.Q_eval.state_dict())\n",
    "                self.target_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\" \n",
    "    this method converts rgb images to grayscale.\n",
    "    \"\"\"\n",
    "    gray = np.dot(rgb[...,:3], [0.2125, 0.7154, 0.0721])\n",
    "    return gray.astype('float32') \n",
    "def test_preprocess(state):\n",
    "    state = state[:-12:,6:-6]\n",
    "    state = rgb2gray(state)\n",
    "    state = state / 255\n",
    "    state = state  * 2 - 1\n",
    "    \n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def grab_test_history(state,state_history,history_length): \n",
    "    if len(state_history) >= history_length:\n",
    "        state_input = np.array(state_history[-history_length:])\n",
    "\n",
    "    \n",
    "    else:\n",
    "        remainder = history_length - len(state_history)\n",
    "        padding = np.zeros(shape = (remainder,84,84))\n",
    "        state_input = np.array(state_history[-history_length:])\n",
    "        state_input = np.concatenate((padding,state_input))\n",
    "        \n",
    "    state_input = torch.from_numpy(state_input).resize(1,history_length,84,84).float()\n",
    "    return state_input    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "C_srL2u2k3Af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1095..1379 -> 284-tiles track\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [16], [16, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6f13058cd6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoreTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#observation = observation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-7bb3be4a6496>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mterminal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPS_DEC\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [16], [16, 3]"
     ]
    }
   ],
   "source": [
    "# Now the main!\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "agent = Agent(gamma=.9, epsilon=.999, alpha=.001,history_length=3, batch_size=16,ch=4, n_actions=3,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=0.996,target_update=100)\n",
    "scores = []\n",
    "eps_history = []\n",
    "n_games = 500\n",
    "score = 0\n",
    "for i in range(n_games):\n",
    "    if i % 10 and i > 0:\n",
    "        avg_score = np.mean(scores[max(0,i-10):(i+1)])\n",
    "        print(\"Episode i\", i, 'average score', avg_score)\n",
    "        \n",
    "        \n",
    "    score = 0 \n",
    "    eps_history.append(agent.EPSILON)\n",
    "    state_history = []\n",
    "    state = env.reset()\n",
    "    state = test_preprocess(state)\n",
    "    state_history.append(state)\n",
    "    state_input = grab_test_history(state,state_history,history_length=3)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = agent.chooseAction(state_input)\n",
    "        state,reward,done, _ = env.step(action)\n",
    "\n",
    "        state = test_preprocess(state)\n",
    "\n",
    "        state_history.append(state)\n",
    "        old_state_input = state_input\n",
    "        \n",
    "        state_input = grab_test_history(state,state_history,history_length=3)\n",
    "        \n",
    "        agent.storeTransition(old_state_input,action,reward, state_input, done)\n",
    "        agent.learn()\n",
    "        \n",
    "        #observation = observation_\n",
    "        score += reward\n",
    "\n",
    "    scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-77-7bb3be4a6496>\u001b[0m(144)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    142 \u001b[0;31m            \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    143 \u001b[0;31m            \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 144 \u001b[0;31m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mterminal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    145 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    146 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPS_DEC\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q_next\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0343,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0346,  0.4978,  0.5059],\n",
      "        [-0.0345,  0.4978,  0.5056],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5057],\n",
      "        [-0.0345,  0.4978,  0.5059],\n",
      "        [-0.0345,  0.4978,  0.5057]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL1M9wDpk3IZ"
   },
   "outputs": [],
   "source": [
    "#Extra credit: Use the polyak average as the weight update rule, as opposed to a variably lagged target. \n",
    "\n",
    "def polyak_update(polyak_factor, target_network, network):\n",
    "    for target_param, param in zip(target_network.parameters(), network.parameters()):\n",
    "        target_param.data.copy_(polyak_factor*param.data + target_param.data*(1.0 - polyak_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3dKz6a3k3OY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSiIj2vdk3Rr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvfk8dhGk3XC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2HtKorhk3Vg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x9-dwFek298"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Load CarRacing In Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
